{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bfaf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "import textwrap\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words as nltk_words, wordnet\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb41032",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This paragraph includes verbs in various tenses (e.g., \"running\", \"hunted\", \"howling\", \"studying\", \"included\", \"documenting\", \"surviving\"), nouns with plural/singular forms, and derived forms (e.g., \"aggression\", \"aggressive\", \"classified\", \"classifying\")—perfect for seeing the effects of stemming and lemmatization.\n",
    "\n",
    "some_text = \"The researchers were studying the behaviors of wolves that had been running, hunted, and howling throughout the forested regions. Interestingly, they noticed that the wolves’ activities varied depending on the season, with increased aggression observed during mating periods. The observation included analyzing journals written by those documenting the wolves’ movements and strategies for surviving in harsh environments. Understanding these interactions helps in predicting future behavioral patterns and classifying different subspecies accordingly.\"\n",
    "\n",
    "\n",
    "# Tokenization using NLTK\n",
    "nltk_tokens_with_punct = word_tokenize(some_text)\n",
    "nltk_tokens_wout_punct = [token for token in nltk_tokens_with_punct if token not in string.punctuation]\n",
    "\n",
    "# Tokenization using spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(some_text)\n",
    "spacy_tokens_with_punct = [token.text for token in doc]\n",
    "spacy_tokens_wout_punct = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# ''.join([f'({token})' for token in nltk_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e0ead",
   "metadata": {},
   "source": [
    "## Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializar ferramentas\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenizar com NLTK e remover pontuações\n",
    "# nltk_tokens = word_tokenize(some_text)\n",
    "# nltk_tokens_wout_punct = [token for token in nltk_tokens if token not in string.punctuation]\n",
    "\n",
    "# Tokenizar com spaCy e criar dicionário {token: lemma}\n",
    "# doc = nlp(some_text)\n",
    "# spacy_token_dict = {token.text: token.lemma_ for token in doc if not token.is_punct and not token.is_space}\n",
    "# nltk_tokens_with_punct = word_tokenize(some_text)\n",
    "# nltk_tokens_wout_punct = [token for token in nltk_tokens_with_punct if token not in string.punctuation]\n",
    "\n",
    "# Filtrar tokens comuns\n",
    "# common_tokens = nltk_tokens_wout_punct #sorted(set(nltk_tokens_wout_punct).intersection(spacy_token_dict.keys()))\n",
    "\n",
    "# Criar DataFrame com todas as colunas comparativas\n",
    "df = pd.DataFrame({\n",
    "    \"Token\": nltk_tokens_wout_punct,\n",
    "    \"Porter Stem\": [porter.stem(token) for token in nltk_tokens_wout_punct],\n",
    "    \"Lancaster Stem\": [lancaster.stem(token) for token in nltk_tokens_wout_punct],\n",
    "    \"Snowball Stem\": [snowball.stem(token) for token in nltk_tokens_wout_punct],\n",
    "    # \"WordNet Lemma\": [lemmatizer.lemmatize(token) for token in nltk_tokens_wout_punct],\n",
    "    # \"spaCy Lemma\": [spacy_token_dict[token] for token in nltk_tokens_wout_punct]\n",
    "})\n",
    "\n",
    "# Exibir\n",
    "HTML(df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2e3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
