{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "import textwrap\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words as nltk_words, wordnet\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276965f3",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This paragraph includes verbs in various tenses (e.g., \"running\", \"hunted\", \"howling\", \"studying\", \"included\", \"documenting\", \"surviving\"), nouns with plural/singular forms, and derived forms (e.g., \"aggression\", \"aggressive\", \"classified\", \"classifying\")—perfect for seeing the effects of stemming and lemmatization.\n",
    "\n",
    "some_text = \"The researchers were studying the behaviors of wolves that had been running, hunted, and howling throughout the forested regions. Interestingly, they noticed that the wolves’ activities varied depending on the season, with increased aggression observed during mating periods. The observation included analyzing journals written by those documenting the wolves’ movements and strategies for surviving in harsh environments. Understanding these interactions helps in predicting future behavioral patterns and classifying different subspecies accordingly.\"\n",
    "\n",
    "\n",
    "# Tokenization using NLTK\n",
    "nltk_tokens_with_punct = word_tokenize(some_text)\n",
    "nltk_tokens_wout_punct = [token for token in nltk_tokens_with_punct if token not in string.punctuation]\n",
    "\n",
    "# Tokenization using spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(some_text)\n",
    "spacy_tokens_with_punct = [token.text for token in doc]\n",
    "spacy_tokens_wout_punct = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# ''.join([f'({token})' for token in nltk_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e42d1",
   "metadata": {},
   "source": [
    "## Lemmatization with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd212769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Função auxiliar para converter POS tags do NLTK para WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "\n",
    "    \"\"\"\n",
    "    This function is a POS tag mapper. It converts Penn Treebank tags (used by nltk.pos_tag) to the simplified tags that WordNetLemmatizer expects.\n",
    "    WordNet only recognizes: wordnet.NOUN, wordnet.VERB, wordnet.ADJ, wordnet.ADV\n",
    "    Thus, this function is useful if you're using NLTK + WordNetLemmatizer to accurately lemmatize words.\n",
    "\n",
    "    Without this function, the lemmatizer WordNetLemmatizer.lemmatize() method defaults to noun (wordnet.NOUN):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer.lemmatize(\"running\", pos=\"v\")  # Output: 'run'\n",
    "    lemmatizer.lemmatize(\"running\")           # Output: 'running'\n",
    "    \"\"\"\n",
    "\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk_pos_tags = pos_tag(nltk_tokens_wout_punct)\n",
    "\n",
    "# spaCy\n",
    "spacy_token_dict = {token.text: token.lemma_ for token in doc if not token.is_punct and not token.is_space}\n",
    "\n",
    "common_tokens = sorted(set([token for token, _ in nltk_pos_tags]).intersection(spacy_token_dict.keys()))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Token\": common_tokens,\n",
    "    # \"POS Tag\": [dict(nltk_pos_tags).get(token) for token in common_tokens],\n",
    "    \"NLTK Lemma (WordNet)\": [lemmatizer.lemmatize(token) for token in common_tokens],\n",
    "    \"NLTK Lemma (WordNet, POS-aware)\": [lemmatizer.lemmatize(token, get_wordnet_pos(dict(nltk_pos_tags).get(token))) for token in common_tokens],\n",
    "    \"spaCy Lemma\": [spacy_token_dict[token] for token in common_tokens]\n",
    "})\n",
    "\n",
    "HTML(df.to_html(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
